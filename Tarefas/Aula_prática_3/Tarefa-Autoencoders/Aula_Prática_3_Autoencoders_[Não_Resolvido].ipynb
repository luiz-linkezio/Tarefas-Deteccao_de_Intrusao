{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aIuOKKe4b_C"
      },
      "source": [
        "# Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztquPfgxg--4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbV8ifz-z-KB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1yw8sDFYv8x"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hhnz3SFgzJ4"
      },
      "outputs": [],
      "source": [
        "sns.set_theme()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc9cX6OHxvJF"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 33\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRuGtYXVuLV2"
      },
      "source": [
        "# Download CIC IDS 2017"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaLg4YymDMSx"
      },
      "source": [
        "[Este](https://www.unb.ca/cic/datasets/ids-2017.html) conjunto de dados contém informações sobre fluxos de rede, representando tanto o tráfego benigno quanto ataques populares. Para o ambiente de teste desta coleta de dados, uma rede foi configurada para o atacante e uma rede separada foi configurada para as vítimas, esta última contendo firewalls, roteadores, switches, servidores e estações de trabalho em execução em diferentes versões dos sistemas operacionais Windows e Linux. Para gerar o tráfego benigno, os autores utilizaram um sistema desenvolvido por eles mesmos, que então extraiu o comportamento abstrato de 25 usuários com base em diferentes protocolos de aplicação. Os dados foram coletados ao longo de cinco dias de atividade de rede e foram processados para extrair mais de 80 features do conjunto de dados usando a ferramenta CICFlowMeter8.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLFSlAwY8zh8"
      },
      "outputs": [],
      "source": [
        "# update gdown version\n",
        "!pip install --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWwpfkp8uM4E"
      },
      "outputs": [],
      "source": [
        "# !wget http://205.174.165.80/CICDataset/CIC-IDS-2017/Dataset/MachineLearningCSV.zip -O CIC_IDS_2017.zip\n",
        "!gdown '1WtbUHBpANHLMVVHuaFr9-pGUyeW6QhdD' -O CIC_IDS_2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdiG5bQNuXK7"
      },
      "outputs": [],
      "source": [
        "# !unzip MachineLearningCSV.zip\n",
        "!unzip ./CIC_IDS_2017.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq_bXXrta-Eo"
      },
      "source": [
        "# Carregando os dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zYbZjA5afwD"
      },
      "outputs": [],
      "source": [
        "df_list = []\n",
        "for file in os.listdir('../../../Dados/MachineLearningCVE/'):\n",
        "  df_aux = pd.read_csv(f'../../../Dados/MachineLearningCVE/{file}')\n",
        "  df_list.append(df_aux)\n",
        "df = pd.concat(df_list, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjM3IcrnalWU"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHn6KCYkcRrD"
      },
      "outputs": [],
      "source": [
        "list(df.columns)[:6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkxtcO7UcZQe"
      },
      "source": [
        "Algumas colunas tem seus nomes iniciados com espaços ou finalizados com espaços. Vamos remover esses espaços não úteis para ajustar o nome das colunas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isiq7uLnL9Sh"
      },
      "outputs": [],
      "source": [
        "df.columns = df.columns.str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-Y0wbj2KHlC"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pawAGo2By0Xz"
      },
      "source": [
        "# Limpando os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKKS5w4tbwoA"
      },
      "source": [
        "É necessário limpar os dados realizando:\n",
        "- Descarte de registros duplicados\n",
        "- Descarte de registros com valores NaN (Not a Number)/ Null / NA (Not Available)\n",
        "- Evitar registros com valores não finitos. Nesse caso, uma abordagem válida é substituirmos os mesmos pelo maior valor finito presente no dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWkkFt-2dPvv"
      },
      "source": [
        "Registros duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sxa5jmZUdRg-"
      },
      "outputs": [],
      "source": [
        "df[df.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zED1GggVxXAg"
      },
      "outputs": [],
      "source": [
        "# Descartando duplicadas\n",
        "initial_len = df.shape[0]\n",
        "df = df.drop_duplicates()\n",
        "print(f'Tamanho inicial: {initial_len}, tamanho final {df.shape[0]} | Descartadas {initial_len - df.shape[0]} duplicadas')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u9Y-ucudcTq"
      },
      "source": [
        "Registros com valores Null/NaN/NA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7LWznKmd_6-"
      },
      "outputs": [],
      "source": [
        "df.columns[df.isna().any(axis=0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiW6qMEyd16h"
      },
      "outputs": [],
      "source": [
        "df[df.isna().any(axis=1)][['Flow Bytes/s']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IT59ZU4NRVf"
      },
      "outputs": [],
      "source": [
        "# Descartando registros com valores NaN/Null/NA\n",
        "initial_len = df.shape[0]\n",
        "df = df.dropna()\n",
        "print(f'Tamanho inicial: {initial_len}, tamanho final {df.shape[0]} | Descartados {initial_len - df.shape[0]} registros com valores NA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq-LwUBfc7pC"
      },
      "outputs": [],
      "source": [
        "df = df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTTh92e0fH2P"
      },
      "source": [
        "Registros com valores não finitos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnUHVexN526E"
      },
      "outputs": [],
      "source": [
        "df_columns_isfinite = np.isfinite(df.drop(['Label'], axis='columns')).all(axis=0)\n",
        "df_columns_isfinite[df_columns_isfinite == False]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1gNjMNPfN1P"
      },
      "outputs": [],
      "source": [
        "df_rows_isfinite = np.isfinite(df.drop(['Label'], axis='columns')).all(axis=1)\n",
        "inf_indexes = df_rows_isfinite[df_rows_isfinite == False].index\n",
        "df.iloc[inf_indexes][['Flow Bytes/s', 'Flow Packets/s', 'Flow Duration']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG1_vSKMf2Oy"
      },
      "outputs": [],
      "source": [
        "# Evitando registros com valores não finitos\n",
        "max_finite_flow_packets_per_sec = df[np.isfinite(df['Flow Packets/s'])]['Flow Packets/s'].max()\n",
        "max_finite_flow_bytes_per_sec = df[np.isfinite(df['Flow Bytes/s'])]['Flow Bytes/s'].max()\n",
        "\n",
        "df.loc[df['Flow Packets/s'] == np.inf, 'Flow Packets/s'] = max_finite_flow_packets_per_sec\n",
        "df.loc[df['Flow Bytes/s'] == np.inf, 'Flow Bytes/s'] = max_finite_flow_bytes_per_sec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-XvDbz6q8jB"
      },
      "source": [
        "# Mini análise exploratória"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVckGuDaxcGO"
      },
      "source": [
        "### Quantidade de instâncias benignas x maliciosas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLbbFtXln9th"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data=df['Label'].apply(lambda label: 'Malicious' if label != 'BENIGN' else 'Benign').to_frame(), x='Label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XomZW4ulyPZI"
      },
      "source": [
        "**Dados não balanceados**. Impactos:\n",
        "- Dificuldade de treinar modelos supervisionados\n",
        "- Dificuldade de avaliar resultados com métricas tradicionais como acurácia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qwei56XxhPf"
      },
      "source": [
        "### Quantidade de instâncias por tipo de ataque"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7aBOsqTzghd"
      },
      "source": [
        "Abaixo está um descritivo para os ataques do dataset:\n",
        "\n",
        "**DoS (Denial of Service)**: Esses ataques, como \"DoS Hulk\", \"DoS GoldenEye\", \"DoS Slowloris\", \"DoS Slowhttptest\" e \"DDoS\" visam tornar temporariamente uma máquina ou recurso de rede indisponível, sendo diferenciados pelo protocolo e estratégia usados para causar a negação de serviço. No caso do \"DDoS\", várias máquinas Windows 8.1 foram usadas para enviar solicitações UDP, TCP e HTTP.\n",
        "\n",
        "**FTP Patator\" e \"SSH Patator**: Usam o software Patator para adivinhar senhas por força bruta com o uso de listas de palavras.\n",
        "\n",
        "**Web - Brute Force**: Usa força bruta em uma aplicação com listas de palavras.\n",
        "\n",
        "**Web - Injeção de SQL**: Esse ataque explora vulnerabilidades em máquinas conectadas publicamente à Internet usando injeção SQL.\n",
        "\n",
        "**Web - XSS (Cross-Site Scripting)**: Representa injeções de scripts em aplicativos da web, visando a execução de ações maliciosas por outros usuários do aplicativo.\n",
        "\n",
        "**PortScan**: Realizados com a ferramenta NMap, esses ataques buscam informações sobre os serviços e portas abertas em um alvo.\n",
        "\n",
        "**Bot**: Esse ataque tem várias possibilidades, como roubo de dados, envio de spam e acesso ao dispositivo. .\n",
        "\n",
        "**Infiltration**: Baseado na infecção de uma máquina após um usuário abrir um arquivo malicioso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9BBNOy1u4aJ"
      },
      "outputs": [],
      "source": [
        "df['Label'] = df['Label'].replace({'Web Attack � Brute Force':'Brute Force', 'Web Attack � XSS':'XSS', 'Web Attack � Sql Injection':'Sql Injection'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSnxTG4Aqsga"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data=df.query('Label != \"BENIGN\"')[['Label']], x='Label', order = df.query('Label != \"BENIGN\"')['Label'].value_counts().index)\n",
        "plt.xticks(rotation=45)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srjop6xWxnWF"
      },
      "source": [
        "Ataques menos representados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HU78W11v-2p"
      },
      "outputs": [],
      "source": [
        "N_LESS_REPRESENTED_LABELS = 5\n",
        "\n",
        "sns.countplot(data=df[df['Label'].isin(df.groupby('Label').size().sort_values(ascending=False)[(-1)*N_LESS_REPRESENTED_LABELS:].index)], x='Label')\n",
        "plt.xticks(rotation=45)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0eln0r6yxbn"
      },
      "source": [
        "### Estatísticas dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL6p6K2Prnr_"
      },
      "outputs": [],
      "source": [
        "interesting_cols = ['Flow Duration', 'Flow Bytes/s', 'Total Fwd Packets', 'Average Packet Size', 'SYN Flag Count']\n",
        "df[interesting_cols].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBSel8WouIw_"
      },
      "source": [
        "# Dividindo dados nos conjuntos de treino, validação e teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHnMjHT8hAU1"
      },
      "source": [
        "**Conjunto de treino**\n",
        "\n",
        "Para a detecção de anomalias, vamos usar somente os dados que representam o tráfego benigno para o conjunto de treino. Dessa forma, os algoritmos de clustering vão ser capazes de identificar padrões e desvios em relação ao comportamento normal (benigno) dos dados.\n",
        "\n",
        "**Conjuntos de validação e teste**\n",
        "\n",
        "Porém, devem ser incluídos dados que representam o tráfego maliciosos nos conjuntos de validação e teste. Esses dados maliciosos no conjunto de validação são importantes para que possamos definir um *threshold* para que seja possível detectar anomalias. Além disso, os dados maliciosos também precisam ser incluídos no conjunto de teste para que possamos avaliar o desempenho do nosso modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI9aRuHROEjH"
      },
      "outputs": [],
      "source": [
        "df_train = df.query('Label == \"BENIGN\"').sample(frac=0.6, random_state=RANDOM_SEED)\n",
        "df_val_test = df.drop(df_train.index)\n",
        "\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "df_val_test = df_val_test.reset_index(drop=True)\n",
        "\n",
        "X_train = df_train.drop('Label', axis='columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZLGwSvtym8X"
      },
      "outputs": [],
      "source": [
        "X_val, X_test, classes_val, classes_test = train_test_split(df_val_test.drop('Label', axis='columns'), df_val_test['Label'], test_size=0.65, stratify=df_val_test['Label'], random_state=RANDOM_SEED)\n",
        "\n",
        "X_val, X_test = X_val.reset_index(drop=True), X_test.reset_index(drop=True)\n",
        "classes_val, classes_test =  classes_val.reset_index(drop=True), classes_test.reset_index(drop=True)\n",
        "\n",
        "y_val, y_test = classes_val.apply(lambda c: 0 if c == 'BENIGN' else 1), classes_test.apply(lambda c: 0 if c == 'BENIGN' else 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFUayxFK7YLr"
      },
      "outputs": [],
      "source": [
        "del df_train, df_val_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrKKFKiM28S2"
      },
      "source": [
        "# Analisando correlação entre features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7GUkxzw0QuI"
      },
      "source": [
        "**Por que remover features?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V25pTEPoVFF"
      },
      "source": [
        "Vamos descartar features com alta correlação evitando passar informações redundantes ao modelo. Dessa forma, conseguiremos obter um modelo mais simples e com menor custo computacional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH8iWIvI8vwu"
      },
      "outputs": [],
      "source": [
        "def get_highly_correlated_features(correlation_matrix, threshold):\n",
        "  correlated_pairs = []\n",
        "  for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "      if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
        "        pair = (correlation_matrix.columns[i], correlation_matrix.columns[j])\n",
        "        coefficient = correlation_matrix.iloc[i, j]\n",
        "        correlated_pairs.append((pair, coefficient))\n",
        "  return sorted(correlated_pairs, key= lambda pair: pair[1], reverse=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYL301wA8msm"
      },
      "outputs": [],
      "source": [
        "corr_matrix = X_train.corr().abs()\n",
        "correlation_list = get_highly_correlated_features(corr_matrix, 0.95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6cC1a08B4Eh"
      },
      "outputs": [],
      "source": [
        "correlation_list[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMe5PnY0-ACu"
      },
      "outputs": [],
      "source": [
        "# Drop high correlated features in correlation list\n",
        "\n",
        "f2drop = []\n",
        "for feature_pair, _ in correlation_list:\n",
        "  if feature_pair[0] not in f2drop and feature_pair[1] not in f2drop:\n",
        "    f2drop.append(feature_pair[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKzy0I0iDgE8"
      },
      "outputs": [],
      "source": [
        "f2drop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czgiFCHwppTO"
      },
      "source": [
        "A feature \"Destination Port\", também não fornece muita contribuição devido que a mesma está codificada com valores inteiros, indicando uma relação de grandeza, como 44720 > 80, que não apresenta sentido semântico quando se trata da porta de destino de um fluxo de rede."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI1Njd7Sd0zo"
      },
      "outputs": [],
      "source": [
        "f2drop = f2drop + ['Destination Port']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqW_NURpDqwy"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.drop(f2drop, axis='columns')\n",
        "X_val = X_val.drop(f2drop, axis='columns')\n",
        "X_test = X_test.drop(f2drop, axis='columns')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0RcHdTfegSU"
      },
      "source": [
        "# Normalizando os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29Tl1aiBqUpP"
      },
      "source": [
        "É importante normalizar os dados para lidar com diferentes escalas, sensibilidades a escalas e até mesmo melhorar o desempenho da convergência dos algoritmos.\n",
        "\n",
        "Caso não seja realizada a normalização, um valor de 10000 para uma feature como \"Flow Bytes/s\" terá impacto similar ao modelo quanto um valor de 10000 para uma feature como \"Flow Packets/s\". Isso é prejudicial, pois o impacto desse valor para as duas features deve ser tratado de forma distinta, já que as mesmas têm escalas e sensibilidades também distintas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgqH-xSneE_1"
      },
      "outputs": [],
      "source": [
        "minmax_scaler = MinMaxScaler()\n",
        "minmax_scaler = minmax_scaler.fit(X_train)\n",
        "\n",
        "norm_X_train = minmax_scaler.transform(X_train)\n",
        "norm_X_val = minmax_scaler.transform(X_val)\n",
        "norm_X_test = minmax_scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chbylf5G05mj"
      },
      "outputs": [],
      "source": [
        "del X_train, X_val, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAlL-F5NnxGg"
      },
      "source": [
        "# Detecção de Anomalias com Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N9Z2na54E2g"
      },
      "source": [
        "## Autoencoder - Explicação\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![Autoencoder](https://tikz.net/janosh/autoencoder.png)\n",
        "</div>\n",
        "\n",
        "Um autoencoder é uma arquitetura de rede neural que aprende a codificar dados em uma representação compacta, chamada de espaço latente, e então reconstruir os dados a partir dessa representação. Ele consiste em duas partes principais: o encoder, que mapeia os dados de entrada para o espaço latente, e o decoder, que reconstrói os dados a partir dessa representação. A ideia central é forçar o modelo a aprender uma representação eficiente e informativa dos dados de entrada.\n",
        "\n",
        "Os autoencoders são frequentemente usados para tarefas de redução de dimensionalidade e denoising. No entanto, eles também são aplicáveis à detecção de anomalias. A lógica é que um autoencoder treinado em dados normais aprenderá a representação latente desses dados, e quando apresentado com dados anômalos que diferem significativamente dos dados normais, a reconstrução será prejudicada, levando a um erro de reconstrução maior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkQLmH5GBn6d"
      },
      "source": [
        "## Relembrando o processo de treinamento de redes neurais - Backpropagation\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![Backpropagation](https://miro.medium.com/v2/resize:fit:640/format:webp/1*VF9xl3cZr2_qyoLfDJajZw.gif)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QunZkF0Q9KM"
      },
      "source": [
        "## Mecanismo de Early Stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yk_8FtDX61T"
      },
      "source": [
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![Early stopping](https://www.researchgate.net/publication/356747729/figure/fig3/AS:1098404738408449@1638891505126/Early-stopping-training-is-stopped-as-soon-as-the-performance-on-the-validation-loss.jpg)\n",
        "</div>\n",
        "\n",
        "O mecanismo de Early Stopping é uma técnica usada durante o treinamento de redes neurais para evitar overfitting e melhorar a eficiência do modelo. O objetivo é interromper o treinamento assim que a performance do modelo em um conjunto de validação começa a piorar, em vez de continuar até que o desempenho no conjunto de treinamento seja perfeito.\n",
        "\n",
        "O mesmo inclui usa dos seguintes argumentos para definir um critério de parada:\n",
        "- **paciência**: Quantidade de épocas limite para esperar melhoria na loss de validação\n",
        "- **delta**: Melhoria mínina necessária para atualizar uma loss de validação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vgQDJed7JF_"
      },
      "outputs": [],
      "source": [
        "# Implementação do Early Stopping\n",
        "class EarlyStopping:\n",
        "  def __init__(self, patience=7, delta=0, verbose=True, path='checkpoint.pt'):\n",
        "      self.patience = patience\n",
        "      self.delta = delta\n",
        "      self.verbose = verbose\n",
        "      self.counter = 0\n",
        "      self.early_stop = False\n",
        "      self.val_min_loss = np.Inf\n",
        "      self.path = path\n",
        "\n",
        "  def __call__(self, val_loss, model):\n",
        "    if val_loss < self.val_min_loss - self.delta:   # Caso a loss da validação reduza, vamos salvar o modelo e nova loss mínima\n",
        "      self.save_checkpoint(val_loss, model)\n",
        "      self.counter = 0\n",
        "    else:                                           # Caso a loss da validação NÃO reduza, vamos incrementar o contador da paciencia\n",
        "      self.counter += 1\n",
        "      print(f'EarlyStopping counter: {self.counter} out of {self.patience}. Current validation loss: {val_loss:.5f}')\n",
        "      if self.counter >= self.patience:\n",
        "          self.early_stop = True\n",
        "\n",
        "  def save_checkpoint(self, val_loss, model):\n",
        "    if self.verbose:\n",
        "        print(f'Validation loss decreased ({self.val_min_loss:.5f} --> {val_loss:.5f}).  Saving model ...')\n",
        "    torch.save(model, self.path)\n",
        "    self.val_min_loss = val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQNY5tphRAQg"
      },
      "source": [
        "## Autoencoder - Implementação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG_t5WfqMjnx"
      },
      "source": [
        "A estrutura básica do autoencoder que vamos montar é:\n",
        "\n",
        "*features de entrada -> 25 -> 10 -> 25 -> features de entrada*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "iP-tDDAFM2Az"
      },
      "outputs": [],
      "source": [
        "# Implementação do Autoencoder\n",
        "class Autoencoder(nn.Module):\n",
        "  def __init__(self, in_features, dropout_rate=0.2, num_layers=4, tamanho_inicial_camada=25, mltply_en_layer_size=0.4):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_features = in_features\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.early_stopping = None\n",
        "    self.num_layers = num_layers\n",
        "    self.tamanho_inicial_camada = tamanho_inicial_camada\n",
        "\n",
        "    # Lista para armazenar as camadas do encoder\n",
        "    encoder_layers = []\n",
        "\n",
        "    # Adiciona camadas ao encoder\n",
        "    for i in range(num_layers//2):\n",
        "        if i == 0:\n",
        "            encoder_layers.append(nn.Linear(in_features, tamanho_inicial_camada))\n",
        "            tamanho_camada = tamanho_inicial_camada\n",
        "            encoder_layers.append(nn.BatchNorm1d(tamanho_camada))\n",
        "            encoder_layers.append(nn.ReLU())\n",
        "            encoder_layers.append(nn.Dropout(dropout_rate))\n",
        "        elif i == (num_layers//2 - 1):\n",
        "            encoder_layers.append(nn.Linear(tamanho_camada, int(tamanho_camada*mltply_en_layer_size)))\n",
        "            tamanho_final_camada = int(tamanho_camada*mltply_en_layer_size)\n",
        "            encoder_layers.append(nn.BatchNorm1d(tamanho_final_camada))\n",
        "            encoder_layers.append(nn.ReLU())\n",
        "        else:\n",
        "            encoder_layers.append(nn.Linear(tamanho_camada, int(tamanho_camada*mltply_en_layer_size)))\n",
        "            tamanho_camada = int(tamanho_camada*mltply_en_layer_size)\n",
        "            encoder_layers.append(nn.BatchNorm1d(tamanho_camada))\n",
        "            encoder_layers.append(nn.ReLU())\n",
        "            encoder_layers.append(nn.Dropout(dropout_rate))\n",
        "\n",
        "    # Define o encoder como uma sequência\n",
        "    self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "\n",
        "    # Lista para armazenar as camadas do decoder\n",
        "    decoder_layers = []\n",
        "    mltply_de_layer_size = 1/mltply_en_layer_size\n",
        "\n",
        "    # Adiciona camadas ao decoder\n",
        "    for i in range(num_layers//2):\n",
        "        if i == 0:\n",
        "            decoder_layers.append(nn.Linear(tamanho_final_camada, math.ceil(int(tamanho_final_camada*mltply_de_layer_size))))\n",
        "            tamanho_camada = math.ceil(int(tamanho_final_camada*mltply_de_layer_size))\n",
        "            decoder_layers.append(nn.BatchNorm1d(tamanho_camada))  # Alterado aqui\n",
        "            decoder_layers.append(nn.ReLU())\n",
        "            decoder_layers.append(nn.Dropout(dropout_rate))\n",
        "        elif i == (num_layers//2 - 1):\n",
        "            decoder_layers.append(nn.Linear(tamanho_camada, in_features))  # Alterado aqui\n",
        "            decoder_layers.append(nn.BatchNorm1d(in_features))  # Alterado aqui\n",
        "            decoder_layers.append(nn.Sigmoid())\n",
        "        else:\n",
        "            decoder_layers.append(nn.Linear(tamanho_camada, math.ceil(int(tamanho_camada*mltply_de_layer_size))))\n",
        "            tamanho_camada = math.ceil(int(tamanho_camada*mltply_de_layer_size))\n",
        "            decoder_layers.append(nn.BatchNorm1d(tamanho_camada))\n",
        "            decoder_layers.append(nn.ReLU())\n",
        "            decoder_layers.append(nn.Dropout(dropout_rate))\n",
        "\n",
        "\n",
        "    # Define o decoder como uma sequência\n",
        "    self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "  def forward(self, X):\n",
        "    encoded = self.encoder(X)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "  def compile(self, learning_rate):\n",
        "    self.criterion = nn.MSELoss()\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
        "\n",
        "  def fit(self, X_train, num_epochs, batch_size, X_val = None, patience = None, delta = None):\n",
        "    if X_val is not None and patience is not None and delta is not None:\n",
        "      print(f'Using early stopping with patience={patience} and delta={delta}')\n",
        "      self.early_stopping = EarlyStopping(patience, delta)\n",
        "\n",
        "    val_avg_losses = []\n",
        "    train_avg_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      # Calibrando os pesos do modelo\n",
        "      train_losses = []\n",
        "      self.train()\n",
        "      for batch in tqdm(range(0, len(X_train), batch_size)):\n",
        "        batch_X = X_train[batch:(batch+batch_size)]\n",
        "        batch_reconstruction = self.forward(batch_X)\n",
        "\n",
        "        train_loss = self.criterion(batch_reconstruction, batch_X)\n",
        "        self.optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        train_losses.append(train_loss.item())\n",
        "      train_avg_loss = np.mean(train_losses)\n",
        "      train_avg_losses.append(train_avg_loss)\n",
        "      print(f'Epoch#{epoch+1}: Train Average Loss = {train_avg_loss:.5f}')\n",
        "\n",
        "      # Mecanismo de early stopping\n",
        "      if self.early_stopping is not None:\n",
        "        val_losses = []\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "          for batch in range(0, len(X_val), batch_size):\n",
        "            batch_X = X_val[batch:(batch+batch_size)]\n",
        "            batch_reconstruction = self.forward(batch_X)\n",
        "            val_loss = self.criterion(batch_reconstruction, batch_X)\n",
        "            val_losses.append(val_loss.item())\n",
        "        val_avg_loss = np.mean(val_losses)\n",
        "        val_avg_losses.append(val_avg_loss)\n",
        "        self.early_stopping(val_avg_loss, self)\n",
        "        if self.early_stopping.early_stop:\n",
        "          print(f'Stopped by early stopping at epoch {epoch+1}')\n",
        "          break\n",
        "\n",
        "    if self.early_stopping is not None:\n",
        "      self = torch.load('checkpoint.pt')\n",
        "    self.eval()\n",
        "    return train_avg_losses, val_avg_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuWULak3W34S"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256\n",
        "LR = 5e-4\n",
        "PATIENCE = 2\n",
        "DELTA = 0.001\n",
        "NUM_EPOCHS = 3\n",
        "IN_FEATURES = norm_X_train.shape[1]\n",
        "DROPOUT_RATE = 0.2\n",
        "NUM_LAYERS = 4 # Digite um número par, se for ímpar será truncado para baixo. Será simétrico, ou seja, metade pro encoder e metade pro decoder\n",
        "INITIAL_LAYER_SIZE = 25 # Tamanho da primeira camada do encoder(não conta com a camada de entrada)\n",
        "MULTIPLIER_ENCODE_LAYER_SIZE = 0.4 # Multiplicador usado para diminuir e aumentar a quantidade de camadas a partir do INITIAL_LAYER_SIZE, lembrando, o tamanho das camadas sempre tem que ser um inteiro, então, valores quebrados no encoder são truncados pra baixo, e no decoder são truncados pra cima(ceil)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "pHbt1yiIPLKC"
      },
      "outputs": [],
      "source": [
        "ae_model = Autoencoder(IN_FEATURES, DROPOUT_RATE, NUM_LAYERS, INITIAL_LAYER_SIZE, MULTIPLIER_ENCODE_LAYER_SIZE)\n",
        "ae_model.compile(learning_rate = LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "WgcIwPvoPUK0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                   [-1, 73]           3,942\n",
            "       BatchNorm1d-2                   [-1, 73]             146\n",
            "              ReLU-3                   [-1, 73]               0\n",
            "           Dropout-4                   [-1, 73]               0\n",
            "            Linear-5                   [-1, 31]           2,294\n",
            "       BatchNorm1d-6                   [-1, 31]              62\n",
            "              ReLU-7                   [-1, 31]               0\n",
            "            Linear-8                   [-1, 71]           2,272\n",
            "       BatchNorm1d-9                   [-1, 71]             142\n",
            "             ReLU-10                   [-1, 71]               0\n",
            "          Dropout-11                   [-1, 71]               0\n",
            "           Linear-12                   [-1, 53]           3,816\n",
            "      BatchNorm1d-13                   [-1, 53]             106\n",
            "          Sigmoid-14                   [-1, 53]               0\n",
            "================================================================\n",
            "Total params: 12,780\n",
            "Trainable params: 12,780\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 0.05\n",
            "Estimated Total Size (MB): 0.06\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "summary(ae_model, (IN_FEATURES,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DHSNXDpW6zU"
      },
      "outputs": [],
      "source": [
        "# Exemplo de treinamento sem utilizar Early Stopping\n",
        "\n",
        "train_avg_losses, _ = ae_model.fit(torch.FloatTensor(norm_X_train), NUM_EPOCHS, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b2dMredGYgB"
      },
      "outputs": [],
      "source": [
        "# Exemplo de treinamento utilizando Early Stopping\n",
        "\n",
        "# Passo 1: Considerar apenas amostras benignas no conjunto de validação\n",
        "benign_norm_X_val = norm_X_val[y_val == 1]\n",
        "benign_norm_X_val = torch.FloatTensor(benign_norm_X_val)\n",
        "\n",
        "# Passo 2: Realizar treinamento do modelo\n",
        "NUM_EPOCHS = 10\n",
        "ae_model_with_es = Autoencoder(IN_FEATURES)\n",
        "ae_model_with_es.compile(learning_rate = LR)\n",
        "train_avg_losses, val_avg_losses = ae_model_with_es.fit(torch.FloatTensor(norm_X_train),\n",
        "                                                NUM_EPOCHS,\n",
        "                                                BATCH_SIZE,\n",
        "                                                X_val = benign_norm_X_val,\n",
        "                                                patience=PATIENCE,\n",
        "                                                delta=DELTA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiG5lN-rZOBA"
      },
      "source": [
        "Abaixo podemos ver um gráfico que exibe as losses (perdas) de treino e validação ao longo das épocas de treinamento. As losses são medidas que indicam quão bem o modelo está aprendendo a tarefa específica para a qual foi treinado. Esse gráfico nos possibilita:\n",
        "\n",
        "- Acompanhar o treinamento\n",
        "- Detectar overfitting\n",
        "- Visualizar a convergência do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNqKLYA6LpSp"
      },
      "outputs": [],
      "source": [
        "def plot_train_val_losses(train_avg_losses, val_avg_losses):\n",
        "  epochs = list(range(1, len(train_avg_losses)+1))\n",
        "  plt.plot(epochs, train_avg_losses, color='blue', label='Loss do treino')\n",
        "  plt.plot(epochs, val_avg_losses, color='orange', label='Loss da validação')\n",
        "  plt.title('Losses de treino e validação por época de treinamento')\n",
        "  plt.legend()\n",
        "\n",
        "plot_train_val_losses(train_avg_losses, val_avg_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLg-YCjABdvU"
      },
      "source": [
        "# Definindo um threshold e avaliando resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NE6rTsK0vg9"
      },
      "outputs": [],
      "source": [
        "def plot_roc_curve(y_true, y_score, max_fpr=1.0):\n",
        "  fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
        "  aucroc = roc_auc_score(y_true, y_score)\n",
        "  plt.plot(100*fpr[fpr < max_fpr], 100*tpr[fpr < max_fpr], label=f'ROC Curve (AUC = {aucroc:.4f})')\n",
        "  plt.xlim(-2,102)\n",
        "  plt.xlabel('FPR (%)')\n",
        "  plt.ylabel('TPR (%)')\n",
        "  plt.legend()\n",
        "  plt.title('ROC Curve and AUCROC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBeDUDwU3gp4"
      },
      "outputs": [],
      "source": [
        "def get_tpr_per_attack(y_labels, y_pred):\n",
        "  aux_df = pd.DataFrame({'Label':y_labels,'prediction':y_pred})\n",
        "  total_per_label = aux_df['Label'].value_counts().to_dict()\n",
        "  correct_predictions_per_label = aux_df.query('Label != \"BENIGN\" and prediction == True').groupby('Label').size().to_dict()\n",
        "  tpr_per_attack = {}\n",
        "  for attack_label, total in total_per_label.items():\n",
        "    if attack_label == 'BENIGN':\n",
        "      continue\n",
        "    tp = correct_predictions_per_label[attack_label] if attack_label in correct_predictions_per_label else 0\n",
        "    tpr = tp/total\n",
        "    tpr_per_attack[attack_label] = tpr\n",
        "  return tpr_per_attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX9dJWs3_6Vi"
      },
      "outputs": [],
      "source": [
        "def get_overall_metrics(y_true, y_pred):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "  acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "  tpr = tp/(tp+fn)\n",
        "  fpr = fp/(fp+tn)\n",
        "  precision = tp/(tp+fp)\n",
        "  f1 = (2*tpr*precision)/(tpr+precision)\n",
        "  return {'acc':acc,'tpr':tpr,'fpr':fpr,'precision':precision,'f1-score':f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A196OkMmC_bI"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  group_counts = [f'{value:.0f}' for value in confusion_matrix(y_true, y_pred).ravel()]\n",
        "  group_percentages = [f'{value*100:.2f}%' for value in confusion_matrix(y_true, y_pred).ravel()/np.sum(cm)]\n",
        "  labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_counts, group_percentages)]\n",
        "  labels = np.array(labels).reshape(2,2)\n",
        "  sns.heatmap(cm, annot=labels, cmap='Oranges', xticklabels=['Predicted Benign', 'Predicted Malicious'], yticklabels=['Actual Benign', 'Actual Malicious'], fmt='')\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD7PVp9NBvjF"
      },
      "source": [
        "## Conjunto de validação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dciHSubefA7H"
      },
      "outputs": [],
      "source": [
        "def get_autoencoder_anomaly_scores(ae_model, X):\n",
        "  X = torch.FloatTensor(X)\n",
        "  reconstructed_X = ae_model(X)\n",
        "  anomaly_scores = torch.mean(torch.pow(X - reconstructed_X, 2), axis=1).detach().numpy() # MSELoss\n",
        "  return anomaly_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvj3-5WDusRM"
      },
      "outputs": [],
      "source": [
        "val_anomaly_scores = get_autoencoder_anomaly_scores(ae_model, norm_X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxBeh1EZ0Mz-"
      },
      "outputs": [],
      "source": [
        "plot_roc_curve(y_val, val_anomaly_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znZDcdRWxeEu"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_val, val_anomaly_scores)\n",
        "df_val_roc = pd.DataFrame({'fpr':fpr, 'tpr':tpr, 'thresholds':thresholds})\n",
        "df_val_roc['youden-index'] = df_val_roc['tpr'] - df_val_roc['fpr']\n",
        "df_val_roc.sort_values('youden-index', ascending=False).drop_duplicates('fpr').query('fpr < 0.03')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aQleJt53bcR"
      },
      "outputs": [],
      "source": [
        "BEST_VALIDATION_THRESHOLD = 0.018680"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuDZOvgPENx5"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_val, val_anomaly_scores > BEST_VALIDATION_THRESHOLD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-EoASOTCCDB"
      },
      "outputs": [],
      "source": [
        "get_overall_metrics(y_val, val_anomaly_scores > BEST_VALIDATION_THRESHOLD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avQKePhJ_EHy"
      },
      "outputs": [],
      "source": [
        "get_tpr_per_attack(classes_val, val_anomaly_scores > BEST_VALIDATION_THRESHOLD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM0PFTPWCAO-"
      },
      "source": [
        "## Conjunto de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfuu8O3L24wa"
      },
      "outputs": [],
      "source": [
        "test_anomaly_scores = get_autoencoder_anomaly_scores(ae_model, norm_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "confJlhE2vlm"
      },
      "outputs": [],
      "source": [
        "plot_roc_curve(y_test, test_anomaly_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS4vFyq2BNWA"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_test, test_anomaly_scores > BEST_VALIDATION_THRESHOLD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvsHNCaUCGR0"
      },
      "outputs": [],
      "source": [
        "get_overall_metrics(y_test, test_anomaly_scores > BEST_VALIDATION_THRESHOLD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79Y707ogCNWO"
      },
      "outputs": [],
      "source": [
        "get_tpr_per_attack(classes_test, test_anomaly_scores > BEST_VALIDATION_THRESHOLD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5Txe3riTfXo"
      },
      "source": [
        "# Pergunta e atividade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KgRe-1rTmIL"
      },
      "source": [
        "## Como um autoencoder é capaz de realizar detecção de anomalias?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMECh0jfBBgj"
      },
      "source": [
        "Insira sua resposta aqui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xeiKdyUT1L8"
      },
      "source": [
        "# Atividade de código"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9X5lBeUT79N"
      },
      "source": [
        "Autoencoders podem ter diferentes arquiteturas de redes neurais para funcionar. O autoencoder visto acima possui as seguintes camadas (representadas juntamente com suas respectivas quantidades de neurônios):\n",
        "\n",
        "*features de entrada -> 25 neurônios -> 10 neurônios -> 25 neurônios -> 10 neurônios*\n",
        "\n",
        "**Crie você mesmo e avalie resultados de um autoencoder com uma nova arquitetura, considerando as seguintes camadas (representadas juntamente com suas respectivas quantidades de neurônios):**\n",
        "\n",
        "**features de entrada -> 30 neurônios -> 20 neurônios -> 10 neurônios -> 20 neurônios -> 30 neurônios -> features de entrada**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY13wtnRWP3R"
      },
      "source": [
        "OBS: Não é necessário tunar os hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8grA5P7Tlff"
      },
      "outputs": [],
      "source": [
        "# Insira seu código aqui\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
